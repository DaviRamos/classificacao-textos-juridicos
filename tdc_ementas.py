# -*- coding: utf-8 -*-
"""tdc-ementas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NyDqwDy8L261Hy5q8-rAjRafM1RW3RDJ

# 1 - Importar depêndencias
"""

!pip install unidecode
!python -m spacy download pt


#pd.set_option('display.max_colwidth', -1)

import pandas as pd
import unidecode as unidecode
import nltk as nltk
import string
import sklearn
from nltk import tokenize
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import Normalizer
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2, f_classif
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
import numpy as np

nltk.download('rslp')
nltk.download('stopwords')

"""# Montar o datafrase"""

uri_data_civel = 'https://github.com/julianopacheco/classificacao-textos-juridicos/blob/master/arquivos/xls/civel.xlsx?raw=true'
df_civel = pd.read_excel(uri_data_civel, nrows=6000)#lendo somente 300 processos
df_civel['CLASSE'] = 0

uri_data_crime = 'https://github.com/julianopacheco/classificacao-textos-juridicos/blob/master/arquivos/xls/crime.xlsx?raw=true'
#uri_data_crime = 'https://github.com/julianopacheco/classificacao-textos-juridicos/blob/master/arquivos/xls/crime_cinco_registros.xlsx?raw=true'
df_crime = pd.read_excel(uri_data_crime, nrows=6000)#lendo somente 3000 processos
df_crime['CLASSE'] = 1

df = pd.concat([df_civel, df_crime], axis=0, ignore_index=True)
df = shuffle(df)

"""# 2 - Converte o campo a processar para uma lista de ementas"""

pd.set_option('display.max_colwidth', -1)
ementas = df['EMENTA'] # Lê a coluna EMENTA e passa para a variável ementas
y = df['CLASSE']	# Lê a coluna CLASSE do dataframe e passa para a variável y

ementas[:5]

"""# Pre-processamento"""

import string
   
def remove_caput(text):
        
    words = text.split()
    
    sem_caput = []
    for w in words:
        if not w.isupper():
            sem_caput.append(w)
    
    result = ''
    for w in sem_caput:
        result = result + w + ' '
    return result

dispositivos = ementas.apply(lambda x: remove_caput(x)) # verbetação + dispositivo
dispositivos.head(3)

df['DISPOSITIVOS'] = dispositivos

df['DISPOSITIVOS'].head(3)

def remover_stopwords_e_aplicar_stemmer(tokenizador, textos):
  stop_words = nltk.corpus.stopwords.words("portuguese")
  ementas_processadas = list()
  for ementa in textos:
    nova_ementa = list()
    print(ementa)
    palavras_ementa = tokenizador.tokenize(ementa)
    print(palavras_ementa)
    for palavra in palavras_ementa:
      if palavra.lower() not in stop_words:
        #nova_ementa.append(stemmer.stem(palavra))
        nova_ementa.append(palavra)
    ementas_processadas.append(' '.join(nova_ementa))
  return ementas_processadas

dispostivos_proc = [unidecode.unidecode(texto.lower()) for texto in df["DISPOSITIVOS"]]
df["DISPOSITIVOS_PROC"] = dispostivos_proc

stemmer = nltk.RSLPStemmer()
punct_tokenize = tokenize.WordPunctTokenizer()

df['DISPOSITIVOS_PROC'] = remover_stopwords_e_aplicar_stemmer(punct_tokenize, df["DISPOSITIVOS_PROC"])

"""# Geração do bag of words"""

count_vectorizer = CountVectorizer()

bag_of_words = count_vectorizer.fit_transform(df['DISPOSITIVOS_PROC'])

random_state_seed = 42

"""# Quebrando o conjunto de treino e teste"""

X_treino, X_teste, y_treino, y_teste = train_test_split(bag_of_words,
                                                              df.CLASSE,
                                                              test_size=0.2,
                                                              random_state = random_state_seed)

"""# Modelos"""

#MultinomialNB
nb = MultinomialNB()

#SVM (Linear):
#=============
svm = SVC(kernel='linear', probability=True)

#DecisionTreeClassifier (DT-CART)
#======================
dtre = DecisionTreeClassifier(max_depth=5)

#Random Forest:
#==============
rf = RandomForestClassifier(n_estimators=40)

#Logistic Regression:
#========================
#lr = LogisticRegression(solver='lbfgs')
lr = LogisticRegression(solver='lbfgs', multi_class='auto',)

#KNN ( KNeighborsClassifier )
neigh = KNeighborsClassifier(n_neighbors=5)

models = [nb,dtre,rf,lr,neigh,svm] 
names = ['NB', 'Descision Tree','Random Florest', 'Logistic Regression', 'KNeighborsClassifier', 'SVM - Linear']

"""# Classificação"""

def executa_algoritmos(X_treino, y_treino, X_teste, y_teste, bag_of_words, models):
  
  for model in models:
    
    print(model)
    model.fit(X_treino,y_treino)
    #print(" Score : ", model.score(X_treino,y_treino))
    y_pred_class =  model.predict(X_teste)
    print(" Accuracy: ", accuracy_score(y_teste, y_pred_class))
    print(" Recall : ",recall_score(y_teste, y_pred_class, average='macro'))
    print(" Precision : ",precision_score(y_teste, y_pred_class, average='macro'))
    print(" f1 socre : ", f1_score(y_teste, y_pred_class, average='macro'))

    target_names = ['CIVIL', 'CRIME']
    print(classification_report(y_teste, y_pred_class, target_names=target_names))
    print("----------------------------------------------------------------------")

  seed = random_state_seed
  # prepare models
  models = []
  models.append(('NB.', nb))
  models.append(('Decision T.', dtre))
  models.append(('RF', rf))
  models.append(('LR', lr))
  models.append(('KNN', neigh))
  models.append(('SVM', svm))
 
  # evaluate each model in turn
  results = []
  names = []
  scoring = 'accuracy'
  for name, model in models:
    kfold = model_selection.KFold(n_splits=10, random_state=seed)
    cv_results = model_selection.cross_val_score(model, bag_of_words, 
                                                 y, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)
  # boxplot algorithm comparison
  fig = plt.figure(num=None, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')
  fig.suptitle('Comparação dos algoritmos')
  #plt.xticks(np.arange(10,30,50,60,80,90,95, 98, 1.00))
  ax = fig.add_subplot(111)
  plt.boxplot(results)
  ax.set_xticklabels(names)
  plt.show()

executa_algoritmos(X_treino, y_treino, X_teste, y_teste, bag_of_words, models)

"""# Curva de aprendizado"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.datasets import load_digits
from sklearn.model_selection import learning_curve
from sklearn.model_selection import ShuffleSplit


def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
   
    plt.figure()
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Training examples")
    plt.ylabel("Score")
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    plt.grid()

    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1,
                     color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
             label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
             label="Cross-validation score")

    plt.legend(loc="best")
    return plt


for model, name in zip(models,names) :

  title = "Learning Curves " + name
  
  # Cross validation with 100 iterations to get smoother mean test and train
  # score curves, each time with 20% data randomly selected as a validation set.
  
  cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
  plot_learning_curve(model, title, X_treino, y_treino, ylim=(0.7, 1.01), cv=cv, n_jobs=4)


plt.show()

"""# Matriz de confusão"""

import itertools  
from sklearn.metrics import confusion_matrix

class_names = ['CIVIL', 'CRIME']

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('Verdadeiro')
    plt.xlabel('Predito')

for model in models:

    y_pred = model.predict(X_teste)    
    
    print(str(model)+'_modelo')
    # Compute confusion matrix
    cnf_matrix = confusion_matrix(y_teste, y_pred)
    np.set_printoptions(precision=2)

    # Plot non-normalized confusion matrix
    plt.figure()
    plot_confusion_matrix(cnf_matrix, classes=class_names,
                          title='Confusion matrix, without normalization')

    # Plot normalized confusion matrix
    #plt.figure()
    #plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,
    #                      title='Normalized confusion matrix')

    plt.show()