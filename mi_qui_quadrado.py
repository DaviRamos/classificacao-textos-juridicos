# -*- coding: utf-8 -*-
"""mi_qui_quadrado.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ObJT0AGJTF46-BNmIUHzyh_lL_psMj1B
"""

!pip install unidecode

"""efetua os imports das dependencias"""

import pandas as pd
import unidecode as unidecode
import nltk as nltk
import string
import sklearn
from nltk import tokenize
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import Normalizer
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2, f_classif

"""efetua downloads de bibliotecas"""

nltk.download('rslp')
nltk.download('stopwords')

print(nltk.__version__)

print(pd.__version__)

print(sklearn.__version__)

from platform import python_version
print(python_version())



"""importa os dados do arquivo excel referente ao civil e adiciona a coluna CLASSE com o valor 0, representando o tipo de dado civil"""

uri_data_civel = 'https://github.com/julianopacheco/classificacao-textos-juridicos/blob/master/arquivos/xls/civel.xlsx?raw=true'
#uri_data_civel = 'https://github.com/julianopacheco/classificacao-textos-juridicos/blob/master/arquivos/xls/civel_cinco_registros.xlsx?raw=true'
dados_civil = pd.read_excel(uri_data_civel)
dados_civil["CLASSE"] = 0

"""importa os dados do arquivo excel referente ao crime e adiciona a coluna CLASSE com o valor 1, representando o tipo de dado crime"""

uri_data_crime = 'https://github.com/julianopacheco/classificacao-textos-juridicos/blob/master/arquivos/xls/crime.xlsx?raw=true'
#uri_data_crime = 'https://github.com/julianopacheco/classificacao-textos-juridicos/blob/master/arquivos/xls/crime_cinco_registros.xlsx?raw=true'
dados_crime = pd.read_excel(uri_data_crime)
dados_crime["CLASSE"] = 1

def removeCaput(text):
        
    words = text.split()
    
    sem_caput = []
    for w in words:
        if not w.isupper():
            sem_caput.append(w)
    
    result = ''
    for w in sem_caput:
        result = result + w + ' '
    return result

"""concatena as duas fontes de dados e gera uma nova contendo as duas"""

dados_civil_e_crime = pd.concat([dados_civil, dados_crime], axis=0, sort=False)

"""como agora a fonte de dados contem as duas listas e necessário embaralhar elas para que o civil e crime nao venha na ordem que foram concatenados

isso e feito com o shuffle
"""

dados_civil_e_crime = shuffle(dados_civil_e_crime)

dados_civil_e_crime_sem_caput = [removeCaput(texto) for texto in dados_civil_e_crime["EMENTA"]]
dados_civil_e_crime["EMENTA_SEM_CAPUT"] = dados_civil_e_crime_sem_caput

dados_civil_e_crime["EMENTA_SEM_CAPUT"].head()

"""verifica a proporcionalidade de cada um dos tipos (civil = 0, crime = 1)"""

dados_civil_e_crime.CLASSE.value_counts()

"""define metodos abaixo para testar a regressão logistica e o svc

cria a funcao para remover as stop words que também aplica o stemmer nas palavras
"""

def remove_stopwords_and_apply_stemmer(tokenizador, textos):
  stop_words = nltk.corpus.stopwords.words("portuguese")
  ementas_processadas = list()
  for ementa in textos:
    nova_ementa = list()
    print(ementa)
    palavras_ementa = tokenizador.tokenize(ementa)
    print(palavras_ementa)
    for palavra in palavras_ementa:
      if palavra.lower() not in stop_words:
        #nova_ementa.append(stemmer.stem(palavra))
        nova_ementa.append(palavra)
    ementas_processadas.append(' '.join(nova_ementa))
  return ementas_processadas

"""define o metodo que aplica e escreve os resultados do algortimo LogisticRegression"""

def execute_LogisticRegression(treino, classe_treino, teste, classe_teste):
  regressao_logistica = LogisticRegression(solver='lbfgs', max_iter=10000)
  regressao_logistica.fit(treino, classe_treino)
  acuracia = regressao_logistica.score(teste, classe_teste)
  print(f'LogisticRegression acuracia: {acuracia}')
  predicted = regressao_logistica.predict(teste)
  print(classification_report(classe_teste, predicted))
  print(confusion_matrix(classe_teste, predicted))

"""define um método que cria um pipeline para normalizar os dados e instanciar o algortimo SVC

efetua o treino, aplica o metodo que irá prever o teste e escreve o relatório da aplicação
"""

def execute_SVC(treino, classe_treino, teste, classe_teste):
  pipeline = Pipeline([
#    ('normalizer', Normalizer()),
    ('svc', SVC(gamma='auto'))
  ])
  pipeline.fit(treino, classe_treino)
  acuracia = pipeline.score(teste, classe_teste)
  print(f'SVC acuracia: {acuracia}')
  predicted = pipeline.predict(teste)
  print(classification_report(classe_teste, predicted))
  print(confusion_matrix(classe_teste, predicted))

"""aplica um pré-processamento nas ementas para remover acentos e adiciona uma nova coluna com estas ementas processadas no dataframe"""

ementas_trat_1 = [unidecode.unidecode(texto.lower()) for texto in dados_civil_e_crime["EMENTA_SEM_CAPUT"]]
#ementas_trat_1 = [unidecode.unidecode(texto.lower()) for texto in dados_civil_e_crime["EMENTA"]]
dados_civil_e_crime["EMENTA_SEM_CAPUT_TRAT_1"] = ementas_trat_1

dados_civil_e_crime["EMENTA_SEM_CAPUT_TRAT_1"].head()

"""instancia o stemmer e o tokenizador baseado em puntução"""

stemmer = nltk.RSLPStemmer()
punct_tokenize = tokenize.WordPunctTokenizer()

"""aplica a remoção das stop words e o stemmer"""

dados_civil_e_crime['EMENTAS_PROCESSADAS'] = remove_stopwords_and_apply_stemmer(punct_tokenize, dados_civil_e_crime["EMENTA_SEM_CAPUT_TRAT_1"])

"""utiliza para a a logica do restante do codigo e aplição dos algortimos as ementas pré-processadas, sem stop words, acentos e com aplição do stemmer"""

dados_civil_e_crime['EMENTAS_PROCESSADAS'].tail()

"""extrai as ementas dos dados"""

ementas = dados_civil_e_crime["EMENTAS_PROCESSADAS"]

"""instancia o CountVectorizer e TfidfVectorizer para vetorizar os textos das ementas"""

count_vectorizer = CountVectorizer(max_features = 12000)

"""cria os bag of words através da transformação com os vetores CountVectorizer e TfidfVectorizer"""

bag_of_words_count = count_vectorizer.fit_transform(ementas)

"""gera as variaveis treino, teste, classe_treino, classe_teste através do train_test_split passando um seed de 48

estas variaveis serão utilizadas abaixo na aplicação dos algortimos
"""

treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words_count,
                                                              dados_civil_e_crime.CLASSE,
                                                              random_state = 48)

"""Implementar logica do Chi2 a partir daqui, o K passado como parametro no SelectKBest, determina a quantidade de palavras que serão selecionadas"""

ch2 = SelectKBest(chi2, k=2000)
X_train = ch2.fit_transform(treino, classe_treino)
X_test = ch2.transform(teste)

"""reupera todas as palavras do vetor"""

feature_names = count_vectorizer.get_feature_names()
pvalues = ch2.pvalues_
len(pvalues)

"""percorre as palavras e obtem somente as palavras que o SelectKBest selecionou, após isso escreve as palavras"""

# https://www.programiz.com/python-programming/methods/list/sort
# http://deeplearningbook.com.br/as-10-principais-arquiteturas-de-redes-neurais/
# https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463
chi_values = list()
for i in ch2.get_support(indices=True):
  chi_values.append((feature_names[i], pvalues[i], stemmer.stem(feature_names[i])))

#feature_names = [f'{feature_names[i]}, {pvalues[i]}' for i in ch2.get_support(indices=True)]

def takeSecond(elem):
    return elem[1]

# print a list of words selected
#chi_values.sort(reverse=True, key=takeSecond)
#for item in chi_values:
#  print(item)

"""executa o algortimo de regressão logistica passando os conjuntos de treino, testes com as palavras selecionadas do qui quadrado"""

execute_LogisticRegression(X_train, classe_treino, X_test, classe_teste)

"""executa o algortimo de SVC passando os conjuntos de treino, testes com as palavras selecionadas do qui quadrado"""

execute_SVC(X_train, classe_treino, X_test, classe_teste)

"""a partir daqui a implementação do mutual information"""

mi = SelectKBest(mutual_info_classif, k=2000)
X_train_mi = mi.fit_transform(treino, classe_treino)
X_test_mi = mi.transform(teste)

#mi.get_support(indices=True)
feature_names_mi = count_vectorizer.get_feature_names()
pvalues_mi = mi.scores_
len(pvalues_mi)

mi_values = list()
for i in mi.get_support(indices=True):
  mi_values.append((feature_names_mi[i], pvalues_mi[i], stemmer.stem(feature_names_mi[i])))

mi_values.sort(reverse=True, key=takeSecond)
for item in mi_values:
  print(item)

"""executa o algortimo de regressão logistica passando os conjuntos de treino, testes com as palavras selecionadas do mutal information"""

execute_LogisticRegression(X_train_mi, classe_treino, X_test_mi, classe_teste)

"""executa o algortimo de SVC passando os conjuntos de treino, testes com as palavras selecionadas do mutual information"""

execute_SVC(X_train_mi, classe_treino, X_test_mi, classe_teste)

fi = SelectKBest(f_classif, k=2000)
X_train_fi = fi.fit_transform(treino, classe_treino)
X_test_fi = fi.transform(teste)

#mi.get_support(indices=True)
feature_names_fi = count_vectorizer.get_feature_names()
pvalues_fi = fi.pvalues_
len(pvalues_fi)

fi_values = list()
for i in mi.get_support(indices=True):
  fi_values.append((feature_names_fi[i], pvalues_fi[i], stemmer.stem(feature_names_fi[i])))

fi_values.sort(reverse=True, key=takeSecond)
for item in fi_values:
  print(item)