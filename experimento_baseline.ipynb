{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from num2words import num2words\n",
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression\n",
    "from sklearn.model_selection import train_test_split # Para dividir o conjunto de treinamento e teste\n",
    "from sklearn.neighbors import KNeighborsClassifier  # K nearest neighbours\n",
    "from sklearn import svm  # Para o algoritmo Support Vector Machine (SVM) Algorithm\n",
    "from sklearn import metrics # Para verificar as métricas\n",
    "from sklearn.tree import DecisionTreeClassifier # para o algoritmo de árvores de decisão\n",
    "from sklearn.neural_network import MLPClassifier # Para as redes neurais\n",
    "from sklearn import ensemble, naive_bayes, neighbors, svm, tree\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beep():\n",
    "    import winsound\n",
    "    duration = 1000  # millisecond\n",
    "    freq = 440  # Hz\n",
    "    #winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metodo para ler o arquivo json recebido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(path, enc='utf8'):\n",
    "    with codecs.open(path, encoding=enc) as j:\n",
    "        data_json = json.load(j)\n",
    "    return data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metodo para remover o index do documento (cabecalho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_document_index(x):\n",
    "    return re.sub('.*(?<=\\\\r1)\\.', '', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método para filtrar as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(tokens):\n",
    "    \"\"\"Docstring.\"\"\"\n",
    "    return [i.lower() for i in tokens if\n",
    "            i.lower() not in stopwords] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método para aplicar o stemming - Note que neste caso a pontuacao foi removida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(x):\n",
    "    return _stemmer.stem(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforma os digitos em texto (e.g. 1 -> um, 2 -> dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_word(word, language = 'pt_BR'):\n",
    "    try:\n",
    "        return num2words(float(word), to = 'cardinal', lang = language)\n",
    "    except NotImplementedError:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrai as metricas do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_metrics(y_predicted, y_test, positive_label=1):\n",
    "        model_metrics = dict()\n",
    "        model_metrics['accuracy'] = metrics.accuracy_score(y_test, y_predicted)\n",
    "        model_metrics['f1'] = metrics.f1_score(y_test, y_predicted, average='weighted', pos_label=1)\n",
    "        model_metrics['precision'] = metrics.precision_score(y_test, y_predicted, average='weighted', pos_label=1)\n",
    "        model_metrics['recall'] = metrics.recall_score(y_test, y_predicted, average='weighted', pos_label=1)\n",
    "\n",
    "        fpr, tpr, threshold = metrics.roc_curve(\n",
    "            y_test, \n",
    "            y_predicted.tolist()\n",
    "        )\n",
    "\n",
    "        # TODO: Rever isso, pois é sobreescrito em seguida\n",
    "        model_metrics['true_positive'] = tpr\n",
    "        model_metrics['false_positive'] = fpr\n",
    "        model_metrics['auc'] = metrics.auc(fpr, tpr)\n",
    "        model_metrics['kappa'] = metrics.cohen_kappa_score(y_test, y_predicted)\n",
    "        model_metrics['log_loss'] = metrics.log_loss(y_test, y_predicted)\n",
    "\n",
    "        confusion_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, y_predicted))\n",
    "\n",
    "        _tp = confusion_matrix.iloc[1,1]\n",
    "        _tn = confusion_matrix.iloc[0,0]\n",
    "        _fp = confusion_matrix.iloc[0,1]\n",
    "        _fn = confusion_matrix.iloc[1,0]\n",
    "\n",
    "        model_metrics['true_positive'] = confusion_matrix.iloc[1,1]\n",
    "        model_metrics['true_negative'] = confusion_matrix.iloc[0,0]\n",
    "        model_metrics['false_positive'] = confusion_matrix.iloc[0,1]\n",
    "        model_metrics['false_negative'] = confusion_matrix.iloc[1,0]\n",
    "\n",
    "        model_metrics['positive_pred_value'] = 0.0 if (_tp + _fp) == 0 else (_tp / (_tp + _fp))\n",
    "        model_metrics['negative_pred_value'] = 0.0 if (_tn + _fn) == 0 else (_tn / (_tn + _fn))\n",
    "\n",
    "        model_metrics['sensitivity'] = 0.0 if (_tp + _fn) == 0 else (_tp / (_tp + _fn))\n",
    "        model_metrics['specificity'] = 0.0 if (_tn + _fp) == 0 else (_tn / (_tn + _fp))\n",
    "\n",
    "        model_metrics['expected_no'] = Counter(y_test)[0]\n",
    "        model_metrics['expected_yes'] = Counter(y_test)[1]\n",
    "        model_metrics['diff_expected'] = abs(Counter(y_test)[1] - Counter(y_predicted)[1])\n",
    "\n",
    "        for x in model_metrics.keys():\n",
    "            model_metrics[x] = float(model_metrics[x])\n",
    "        \n",
    "        return model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definições globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stemmer = PortugueseStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = read_json_file('/Users/julianopacheco/dev/python_workspace/classificacao-textos-juridicos/arquivos/json/civel.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crime = read_json_file('/Users/julianopacheco/dev/python_workspace/classificacao-textos-juridicos/arquivos/json/crime.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crime = data_crime['Documentos']\n",
    "k = list()\n",
    "for x in data_crime:\n",
    "    k.append(remove_document_index(x['EMENTA']))\n",
    "\n",
    "data_crime = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = data_civil['Documentos']\n",
    "k = list()\n",
    "for x in data_civil:\n",
    "    k.append(remove_document_index(x['EMENTA']))\n",
    "\n",
    "data_civil = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove o cabeçalho e caracteres de controle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data_civil)):\n",
    "    x = data_civil[i]\n",
    "    x = re.sub('.*(?<=\\\\r1)\\.', '', x)\n",
    "    x = re.sub(r'[\\t\\n\\r]', ' ', x)\n",
    "    x = re.sub(\"\\s\\s+\", \" \", x)\n",
    "    data_civil[i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data_crime)):\n",
    "    x = data_crime[i]\n",
    "    x = re.sub('.*(?<=\\\\r1)\\.', '', x)\n",
    "    x = re.sub(r'[\\t\\n\\r]', ' ', x)\n",
    "    x = re.sub(\"\\s\\s+\", \" \", x)\n",
    "    data_crime[i] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o filtro por stopwords em todos os textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = [' '.join(filter_stopwords(x.split(' '))) for x in data_civil]\n",
    "data_crime = [' '.join(filter_stopwords(x.split(' '))) for x in data_crime]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o filtro o stemming em todos os textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = [stemming(x) for x in data_civil]\n",
    "data_crime = [stemming(x) for x in data_crime]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria o vectorizer para processar todos os textos de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repassa todos os textos para o TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_civil\n",
    "texts.extend(data_crime)\n",
    "vectorizer.fit(texts)\n",
    "del texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtém o vetor TF-IDF para o primeiro elemento de todos os textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = vectorizer.transform(data_civil).toarray()\n",
    "data_crime = vectorizer.transform(data_crime).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gera os dataframes e atribui os labels dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = pd.DataFrame(data_civil,columns=vectorizer.get_feature_names())\n",
    "data_crime = pd.DataFrame(data_crime,columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil['CLASSE'] = 'civil'\n",
    "data_crime['CLASSE'] = 'crime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([data_civil, data_crime], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = shuffle(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide o conjunto em treinamento (70%) e teste (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['CLASSE'] = [1 if x == 'crime' else 0 for x in all_data['CLASSE']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_data.drop('CLASSE', axis=1, inplace=False), all_data['CLASSE'], test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração da semente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_seed = 2567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o SVM com kernel linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "w3U2_207cc03",
    "outputId": "36057a61-55a3-4cd6-813f-550b816c1433"
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='linear', gamma='auto', C=1, degree=0.1, probability=False, random_state=random_state_seed)\n",
    "model.fit(x_train,y_train) # nós treinamos o algoritmo com os dados de treinamento e a saída de treinamento\n",
    "y_predicted=model.predict(x_test) # agora passamos os dados de teste para o algoritmo treinado\n",
    "\n",
    "# Para verificar o desempenho, é necessário passar a saída obtida pelo modelo e a esperada\n",
    "extract_model_metrics(y_predicted,y_test) # agora nós verificamos a acurácia do algoritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o SVM com kernel radial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4-aLdCD0gqCq",
    "outputId": "654cd800-8e28-4f4a-9724-9d4dcf133c14",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='rbf', gamma='auto', C=1, degree=0.1, probability=False, random_state=random_state_seed)\n",
    "model.fit(x_train,y_train) # nós treinamos o algoritmo com os dados de treinamento e a saída de treinamento\n",
    "y_predicted=model.predict(x_test) # agora passamos os dados de teste para o algoritmo treinado\n",
    "\n",
    "# Para verificar o desempenho, é necessário passar a saída obtida pelo modelo e a esperada\n",
    "extract_model_metrics(y_predicted,y_test) # agora nós verificamos a acurácia do algoritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o SVM com kernel polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Fgxfl9BYg0ar",
    "outputId": "90419813-1d8d-4db1-d70b-b24114972bef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='poly', gamma='auto', C=1, degree=0.1, probability=False, random_state=random_state_seed)\n",
    "model.fit(x_train,y_train) # nós treinamos o algoritmo com os dados de treinamento e a saída de treinamento\n",
    "y_predicted=model.predict(x_test) # agora passamos os dados de teste para o algoritmo treinado\n",
    "\n",
    "# Para verificar o desempenho, é necessário passar a saída obtida pelo modelo e a esperada\n",
    "extract_model_metrics(y_predicted,y_test) # agora nós verificamos a acurácia do algoritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica a regressão logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "syAGbrjkcc3g",
    "outputId": "740bf214-1fbb-43cb-d712-c98a965edbd2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=random_state_seed)\n",
    "model.fit(x_train,y_train)\n",
    "y_predicted=model.predict(x_test)\n",
    "extract_model_metrics(y_predicted,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica árvores de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "p1z6BoYDcc61",
    "outputId": "f9b84f30-9139-41ca-8c11-84eafdbd7376",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=DecisionTreeClassifier(random_state=random_state_seed)\n",
    "model.fit(x_train,y_train)\n",
    "y_predicted=model.predict(x_test)\n",
    "extract_model_metrics(y_predicted,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o algoritmo do KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hGCgpjG2hAUO",
    "outputId": "af776645-8495-4748-f594-7e093f8bf048",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(x_train,y_train)\n",
    "y_predicted=model.predict (x_test)\n",
    "extract_model_metrics(y_predicted,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o algoritmo Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gCCQH6fxcdBq",
    "outputId": "7557e308-6396-4913-c54e-10149605ecab"
   },
   "outputs": [],
   "source": [
    "model=ensemble.RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=None, n_jobs=-1, bootstrap=True, random_state=random_state_seed)\n",
    "model.fit(x_train,y_train)\n",
    "y_predicted=model.predict(x_test)\n",
    "extract_model_metrics(y_predicted,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o algoritmo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0J3UPFGbhAmw",
    "outputId": "321e2200-cc5f-4f04-9e1b-cfce45fb5093"
   },
   "outputs": [],
   "source": [
    "model=naive_bayes.GaussianNB()\n",
    "model.fit(x_train,y_train)\n",
    "y_predicted=model.predict(x_test)\n",
    "extract_model_metrics(y_predicted,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
