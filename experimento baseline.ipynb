{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "from num2words import num2words\n",
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\allanbs\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "c:\\users\\allanbs\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression\n",
    "from sklearn.cross_validation import train_test_split # Para dividir o conjunto de treinamento e teste\n",
    "from sklearn.neighbors import KNeighborsClassifier  # K nearest neighbours\n",
    "from sklearn import svm  # Para o algoritmo Support Vector Machine (SVM) Algorithm\n",
    "from sklearn import metrics # Para verificar as métricas\n",
    "from sklearn.tree import DecisionTreeClassifier # para o algoritmo de árvores de decisão\n",
    "from sklearn.neural_network import MLPClassifier # Para as redes neurais\n",
    "from sklearn import ensemble, naive_bayes, neighbors, svm, tree\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beep():\n",
    "    import winsound\n",
    "    duration = 1000  # millisecond\n",
    "    freq = 440  # Hz\n",
    "    winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metodo para ler o arquivo json recebido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(path, enc='utf8'):\n",
    "    with codecs.open(path, encoding=enc) as j:\n",
    "        data_json = json.load(j)\n",
    "    return data_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metodo para remover o index do documento (cabecalho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_document_index(x):\n",
    "    return re.sub('.*(?<=\\\\r1)\\.', '', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método para filtrar as stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(tokens):\n",
    "    \"\"\"Docstring.\"\"\"\n",
    "    return [i.lower() for i in tokens if\n",
    "            i.lower() not in stopwords] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método para aplicar o stemming - Note que neste caso a pontuacao foi removida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(x):\n",
    "    return _stemmer.stem(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforma os digitos em texto (e.g. 1 -> um, 2 -> dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_word(word, language = 'pt_BR'):\n",
    "    try:\n",
    "        return num2words(float(word), to = 'cardinal', lang = language)\n",
    "    except NotImplementedError:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrai as metricas do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_metrics(y_predicted, y_test, positive_label=1):\n",
    "        model_metrics = dict()\n",
    "        model_metrics['accuracy'] = metrics.accuracy_score(y_test, y_predicted)\n",
    "        model_metrics['f1'] = metrics.f1_score(y_test, y_predicted, average='weighted', pos_label=1)\n",
    "        model_metrics['precision'] = metrics.precision_score(y_test, y_predicted, average='weighted', pos_label=1)\n",
    "        model_metrics['recall'] = metrics.recall_score(y_test, y_predicted, average='weighted', pos_label=1)\n",
    "\n",
    "        fpr, tpr, threshold = metrics.roc_curve(\n",
    "            y_test, \n",
    "            y_predicted.tolist()\n",
    "        )\n",
    "\n",
    "        # TODO: Rever isso, pois é sobreescrito em seguida\n",
    "        model_metrics['true_positive'] = tpr\n",
    "        model_metrics['false_positive'] = fpr\n",
    "        model_metrics['auc'] = metrics.auc(fpr, tpr)\n",
    "        model_metrics['kappa'] = metrics.cohen_kappa_score(y_test, y_predicted)\n",
    "        model_metrics['log_loss'] = metrics.log_loss(y_test, y_predicted)\n",
    "\n",
    "        confusion_matrix = pd.DataFrame(metrics.confusion_matrix(y_test, y_predicted))\n",
    "\n",
    "        _tp = confusion_matrix.iloc[1,1]\n",
    "        _tn = confusion_matrix.iloc[0,0]\n",
    "        _fp = confusion_matrix.iloc[0,1]\n",
    "        _fn = confusion_matrix.iloc[1,0]\n",
    "\n",
    "        model_metrics['true_positive'] = confusion_matrix.iloc[1,1]\n",
    "        model_metrics['true_negative'] = confusion_matrix.iloc[0,0]\n",
    "        model_metrics['false_positive'] = confusion_matrix.iloc[0,1]\n",
    "        model_metrics['false_negative'] = confusion_matrix.iloc[1,0]\n",
    "\n",
    "        model_metrics['positive_pred_value'] = 0.0 if (_tp + _fp) == 0 else (_tp / (_tp + _fp))\n",
    "        model_metrics['negative_pred_value'] = 0.0 if (_tn + _fn) == 0 else (_tn / (_tn + _fn))\n",
    "\n",
    "        model_metrics['sensitivity'] = 0.0 if (_tp + _fn) == 0 else (_tp / (_tp + _fn))\n",
    "        model_metrics['specificity'] = 0.0 if (_tn + _fp) == 0 else (_tn / (_tn + _fp))\n",
    "\n",
    "        model_metrics['expected_no'] = Counter(y_test)[0]\n",
    "        model_metrics['expected_yes'] = Counter(y_test)[1]\n",
    "        model_metrics['diff_expected'] = abs(Counter(y_test)[1] - Counter(y_predicted)[1])\n",
    "\n",
    "        for x in model_metrics.keys():\n",
    "            model_metrics[x] = float(model_metrics[x])\n",
    "        \n",
    "        return model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definições globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stemmer = PortugueseStemmer()\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = read_json_file('C:/Users/allanbs/Documents/Git/RicardoDissertacao/dados/Formato Json/Civel.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crime = read_json_file('C:/Users/allanbs/Documents/Git/RicardoDissertacao/dados/Formato Json/Crime.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crime = data_crime['Documentos']\n",
    "k = list()\n",
    "for x in data_crime:\n",
    "    k.append(remove_document_index(x['EMENTA']))\n",
    "\n",
    "data_crime = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = data_civil['Documentos']\n",
    "k = list()\n",
    "for x in data_civil:\n",
    "    k.append(remove_document_index(x['EMENTA']))\n",
    "\n",
    "data_civil = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove o cabeçalho e caracteres de controle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data_civil)):\n",
    "    x = data_civil[i]\n",
    "    x = re.sub('.*(?<=\\\\r1)\\.', '', x)\n",
    "    x = re.sub(r'[\\t\\n\\r]', ' ', x)\n",
    "    x = re.sub(\"\\s\\s+\", \" \", x)\n",
    "    data_civil[i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data_crime)):\n",
    "    x = data_crime[i]\n",
    "    x = re.sub('.*(?<=\\\\r1)\\.', '', x)\n",
    "    x = re.sub(r'[\\t\\n\\r]', ' ', x)\n",
    "    x = re.sub(\"\\s\\s+\", \" \", x)\n",
    "    data_crime[i] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o filtro por stopwords em todos os textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = [' '.join(filter_stopwords(x.split(' '))) for x in data_civil]\n",
    "data_crime = [' '.join(filter_stopwords(x.split(' '))) for x in data_crime]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o filtro o stemming em todos os textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = [stemming(x) for x in data_civil]\n",
    "data_crime = [stemming(x) for x in data_crime]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria o vectorizer para processar todos os textos de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repassa todos os textos para o TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_civil\n",
    "texts.extend(data_crime)\n",
    "vectorizer.fit(texts)\n",
    "del texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtém o vetor TF-IDF para o primeiro elemento de todos os textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = vectorizer.transform(data_civil).toarray()\n",
    "data_crime = vectorizer.transform(data_crime).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gera os dataframes e atribui os labels dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil = pd.DataFrame(data_civil,columns=vectorizer.get_feature_names())\n",
    "data_crime = pd.DataFrame(data_crime,columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_civil['CLASSE'] = 'civil'\n",
    "data_crime['CLASSE'] = 'crime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([data_civil, data_crime], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = shuffle(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide o conjunto em treinamento (70%) e teste (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['CLASSE'] = [1 if x == 'crime' else 0 for x in all_data['CLASSE']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_data.drop('CLASSE', axis=1, inplace=False), all_data['CLASSE'], test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração da semente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_seed = 2567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o SVM com kernel linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "w3U2_207cc03",
    "outputId": "36057a61-55a3-4cd6-813f-550b816c1433"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4898148148148148,\n",
       " 'auc': 0.4201102034685874,\n",
       " 'diff_expected': 83.0,\n",
       " 'expected_no': 3601.0,\n",
       " 'expected_yes': 1799.0,\n",
       " 'f1': 0.48669763105590147,\n",
       " 'false_negative': 1419.0,\n",
       " 'false_positive': 1336.0,\n",
       " 'kappa': -0.16164559666278122,\n",
       " 'log_loss': 17.621369857434384,\n",
       " 'negative_pred_value': 0.6148208469055375,\n",
       " 'positive_pred_value': 0.22144522144522144,\n",
       " 'precision': 0.4837684857568137,\n",
       " 'recall': 0.4898148148148148,\n",
       " 'sensitivity': 0.2112284602556976,\n",
       " 'specificity': 0.6289919466814774,\n",
       " 'true_negative': 2265.0,\n",
       " 'true_positive': 380.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = svm.SVC(kernel='linear', gamma='auto', C=1, degree=0.1, probability=False, random_state=random_state_seed)\n",
    "model.fit(x_train,y_train) # nós treinamos o algoritmo com os dados de treinamento e a saída de treinamento\n",
    "y_predicted=model.predict(x_test) # agora passamos os dados de teste para o algoritmo treinado\n",
    "\n",
    "# Para verificar o desempenho, é necessário passar a saída obtida pelo modelo e a esperada\n",
    "extract_model_metrics(y_predicted,y_test) # agora nós verificamos a acurácia do algoritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o SVM com kernel radial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4-aLdCD0gqCq",
    "outputId": "654cd800-8e28-4f4a-9724-9d4dcf133c14",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\allanbs\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\allanbs\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6668518518518518,\n",
       " 'auc': 0.5,\n",
       " 'diff_expected': 1799.0,\n",
       " 'expected_no': 3601.0,\n",
       " 'expected_yes': 1799.0,\n",
       " 'f1': 0.5335703851835393,\n",
       " 'false_negative': 1799.0,\n",
       " 'false_positive': 0.0,\n",
       " 'kappa': 0.0,\n",
       " 'log_loss': 11.506529395267465,\n",
       " 'negative_pred_value': 0.6668518518518518,\n",
       " 'positive_pred_value': 0.0,\n",
       " 'precision': 0.4446913923182441,\n",
       " 'recall': 0.6668518518518518,\n",
       " 'sensitivity': 0.0,\n",
       " 'specificity': 1.0,\n",
       " 'true_negative': 3601.0,\n",
       " 'true_positive': 0.0}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = svm.SVC(kernel='rbf', gamma='auto', C=1, degree=0.1, probability=False, random_state=random_state_seed)\n",
    "model.fit(x_train,y_train) # nós treinamos o algoritmo com os dados de treinamento e a saída de treinamento\n",
    "y_predicted=model.predict(x_test) # agora passamos os dados de teste para o algoritmo treinado\n",
    "\n",
    "# Para verificar o desempenho, é necessário passar a saída obtida pelo modelo e a esperada\n",
    "extract_model_metrics(y_predicted,y_test) # agora nós verificamos a acurácia do algoritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o SVM com kernel polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Fgxfl9BYg0ar",
    "outputId": "90419813-1d8d-4db1-d70b-b24114972bef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='poly', gamma='auto', C=1, degree=0.1, probability=False, random_state=random_state_seed)\n",
    "model.fit(x_train,y_train) # nós treinamos o algoritmo com os dados de treinamento e a saída de treinamento\n",
    "y_predicted=model.predict(x_test) # agora passamos os dados de teste para o algoritmo treinado\n",
    "\n",
    "# Para verificar o desempenho, é necessário passar a saída obtida pelo modelo e a esperada\n",
    "extract_model_metrics(y_predicted,y_test) # agora nós verificamos a acurácia do algoritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica a regressão logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "syAGbrjkcc3g",
    "outputId": "740bf214-1fbb-43cb-d712-c98a965edbd2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5185185185185185,\n",
       " 'auc': 0.44218833042949124,\n",
       " 'diff_expected': 230.0,\n",
       " 'expected_no': 3601.0,\n",
       " 'expected_yes': 1799.0,\n",
       " 'f1': 0.5095286499179021,\n",
       " 'false_negative': 1415.0,\n",
       " 'false_positive': 1185.0,\n",
       " 'kappa': -0.11944421986458553,\n",
       " 'log_loss': 16.629956694393066,\n",
       " 'negative_pred_value': 0.6306447402766902,\n",
       " 'positive_pred_value': 0.2447418738049713,\n",
       " 'precision': 0.502081914946575,\n",
       " 'recall': 0.5185185185185185,\n",
       " 'sensitivity': 0.21345191773207337,\n",
       " 'specificity': 0.6709247431269092,\n",
       " 'true_negative': 2416.0,\n",
       " 'true_positive': 384.0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=random_state_seed)\n",
    "model.fit(x_train,y_train)\n",
    "y_predicted=model.predict(x_test)\n",
    "extract_model_metrics(y_predicted,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica árvores de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "p1z6BoYDcc61",
    "outputId": "f9b84f30-9139-41ca-8c11-84eafdbd7376",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.43203703703703705,\n",
       " 'auc': 0.33979312768872955,\n",
       " 'diff_expected': 303.0,\n",
       " 'expected_no': 3601.0,\n",
       " 'expected_yes': 1799.0,\n",
       " 'f1': 0.4173880151013432,\n",
       " 'false_negative': 1685.0,\n",
       " 'false_positive': 1382.0,\n",
       " 'kappa': -0.3345106262558024,\n",
       " 'log_loss': 19.616950416081405,\n",
       " 'negative_pred_value': 0.5683913934426229,\n",
       " 'positive_pred_value': 0.07620320855614973,\n",
       " 'precision': 0.40441981110729597,\n",
       " 'recall': 0.43203703703703705,\n",
       " 'sensitivity': 0.06336853807670928,\n",
       " 'specificity': 0.6162177173007498,\n",
       " 'true_negative': 2219.0,\n",
       " 'true_positive': 114.0}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=DecisionTreeClassifier(random_state=random_state_seed)\n",
    "model.fit(x_train,y_train)\n",
    "y_predicted=model.predict(x_test)\n",
    "extract_model_metrics(y_predicted,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o algoritmo do KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hGCgpjG2hAUO",
    "outputId": "af776645-8495-4748-f594-7e093f8bf048",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4875925925925926,\n",
       " 'auc': 0.42136471880533466,\n",
       " 'diff_expected': 29.0,\n",
       " 'expected_no': 3601.0,\n",
       " 'expected_yes': 1799.0,\n",
       " 'f1': 0.4865383104948638,\n",
       " 'false_negative': 1398.0,\n",
       " 'false_positive': 1369.0,\n",
       " 'kappa': -0.15790746225629904,\n",
       " 'log_loss': 17.69812758029626,\n",
       " 'negative_pred_value': 0.6148760330578512,\n",
       " 'positive_pred_value': 0.22655367231638418,\n",
       " 'precision': 0.48550715769231434,\n",
       " 'recall': 0.4875925925925926,\n",
       " 'sensitivity': 0.22290161200667039,\n",
       " 'specificity': 0.6198278256039988,\n",
       " 'true_negative': 2232.0,\n",
       " 'true_positive': 401.0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(x_train,y_train)\n",
    "y_predicted=model.predict (x_test)\n",
    "extract_model_metrics(y_predicted,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o algoritmo Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gCCQH6fxcdBq",
    "outputId": "7557e308-6396-4913-c54e-10149605ecab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4324074074074074,\n",
       " 'auc': 0.3449386936091342,\n",
       " 'diff_expected': 235.0,\n",
       " 'expected_no': 3601.0,\n",
       " 'expected_yes': 1799.0,\n",
       " 'f1': 0.42154387757039086,\n",
       " 'false_negative': 1650.0,\n",
       " 'false_positive': 1415.0,\n",
       " 'kappa': -0.32060121944369246,\n",
       " 'log_loss': 19.604163163104623,\n",
       " 'negative_pred_value': 0.5698644421272159,\n",
       " 'positive_pred_value': 0.09526854219948849,\n",
       " 'precision': 0.4117536969475896,\n",
       " 'recall': 0.4324074074074074,\n",
       " 'sensitivity': 0.08282379099499722,\n",
       " 'specificity': 0.6070535962232713,\n",
       " 'true_negative': 2186.0,\n",
       " 'true_positive': 149.0}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=ensemble.RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=None, n_jobs=-1, bootstrap=True, random_state=random_state_seed)\n",
    "model.fit(x_train,y_train)\n",
    "y_predicted=model.predict(x_test)\n",
    "extract_model_metrics(y_predicted,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica o algoritmo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0J3UPFGbhAmw",
    "outputId": "321e2200-cc5f-4f04-9e1b-cfce45fb5093"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5031481481481481,\n",
       " 'auc': 0.4739182139974397,\n",
       " 'diff_expected': 475.0,\n",
       " 'expected_no': 3601.0,\n",
       " 'expected_yes': 1799.0,\n",
       " 'f1': 0.5145783949444179,\n",
       " 'false_negative': 1104.0,\n",
       " 'false_positive': 1579.0,\n",
       " 'kappa': -0.04893100000086892,\n",
       " 'log_loss': 17.160888820719933,\n",
       " 'negative_pred_value': 0.6468330134357005,\n",
       " 'positive_pred_value': 0.30562884784520666,\n",
       " 'precision': 0.5331614775287934,\n",
       " 'recall': 0.5031481481481481,\n",
       " 'sensitivity': 0.38632573652028906,\n",
       " 'specificity': 0.5615106914745904,\n",
       " 'true_negative': 2022.0,\n",
       " 'true_positive': 695.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=naive_bayes.GaussianNB()\n",
    "model.fit(x_train,y_train)\n",
    "y_predicted=model.predict(x_test)\n",
    "extract_model_metrics(y_predicted,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
